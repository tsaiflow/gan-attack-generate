{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "In this notebook, we'll be building a generative adversarial network (GAN) trained on the network flow dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kc3172/py3tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, models\n",
    "reload(utils)\n",
    "reload(models)\n",
    "from models import Generator, Discriminator\n",
    "from utils import max_norm, parse_feature_label, train_one_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出的summary的保存目录:不要改\n",
    "OUTPUT_DIR = 'SUMMARY/'\n",
    "\n",
    "# 输出的模型的保存目录:不要改\n",
    "CHECKPOINT_DIR = 'CHECKPOINT/'\n",
    "\n",
    "# 保存data的目录:不要改\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "# 相关feature:不太需要改应该\n",
    "RELEVANT_FEATURES = [' Source Port', ' Destination Port', ' Flow Duration', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', 'Bwd Packet Length Max', ' Bwd Packet Length Min', 'Flow Bytes/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Fwd Header Length', ' Bwd Packets/s', ' Packet Length Mean', ' ACK Flag Count', ' Down/Up Ratio', ' Avg Fwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Bwd Avg Bytes/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' act_data_pkt_fwd', ' Active Std', ' Active Min', ' Idle Max']\n",
    "\n",
    "# Label的名字:不太需要改应该\n",
    "LABEL_NAME = ' Label'\n",
    "\n",
    "# 记录summary的频率:不太需要改应该\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "# 数据集路径\n",
    "IDS_DATASET = os.path.join('../data', 'ids2017_sampled.csv')\n",
    "\n",
    "# benign flow的label\n",
    "BENIGN_LABEL = 0\n",
    "\n",
    "# attack flow的label\n",
    "ATTACK_LABEL = 2\n",
    "\n",
    "# training dataset占总dataset的比例\n",
    "TRAIN_FRAC = 0.3\n",
    "\n",
    "# 要修改的feature的数量\n",
    "FEATURE_NUM_MODIFIED = 35\n",
    "\n",
    "# kc change lr rate from 0.01 to 0.0001\n",
    "# learning rate\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# 训练的epochs数\n",
    "EPOCHS = 100\n",
    "\n",
    "# 是否随机选择feature\n",
    "RANDOM_SELECT_FEATURE=False\n",
    "\n",
    "## 提示：\n",
    "## 训练结束后，把SUMMARY和CHECKPOINT目录重命名后保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow ID, Source IP, Source Port, Destination IP, Destination Port, Protocol, Timestamp, Flow Duration, Total Fwd Packets, Total Backward Packets,Total Length of Fwd Packets, Total Length of Bwd Packets, Fwd Packet Length Max, Fwd Packet Length Min, Fwd Packet Length Mean, Fwd Packet Length Std,Bwd Packet Length Max, Bwd Packet Length Min, Bwd Packet Length Mean, Bwd Packet Length Std,Flow Bytes/s, Flow Packets/s, Flow IAT Mean, Flow IAT Std, Flow IAT Max, Flow IAT Min,Fwd IAT Total, Fwd IAT Mean, Fwd IAT Std, Fwd IAT Max, Fwd IAT Min,Bwd IAT Total, Bwd IAT Mean, Bwd IAT Std, Bwd IAT Max, Bwd IAT Min,Fwd PSH Flags, Bwd PSH Flags, Fwd URG Flags, Bwd URG Flags, Fwd Header Length, Bwd Header Length,Fwd Packets/s, Bwd Packets/s, Min Packet Length, Max Packet Length, Packet Length Mean, Packet Length Std, Packet Length Variance,FIN Flag Count, SYN Flag Count, RST Flag Count, PSH Flag Count, ACK Flag Count, URG Flag Count, CWE Flag Count, ECE Flag Count, Down/Up Ratio, Average Packet Size, Avg Fwd Segment Size, Avg Bwd Segment Size, Fwd Header Length.1,Fwd Avg Bytes/Bulk, Fwd Avg Packets/Bulk, Fwd Avg Bulk Rate, Bwd Avg Bytes/Bulk, Bwd Avg Packets/Bulk,Bwd Avg Bulk Rate,Subflow Fwd Packets, Subflow Fwd Bytes, Subflow Bwd Packets, Subflow Bwd Bytes,Init_Win_bytes_forward, Init_Win_bytes_backward, act_data_pkt_fwd, min_seg_size_forward,Active Mean, Active Std, Active Max, Active Min,Idle Mean, Idle Std, Idle Max, Idle Min, Label\r\n",
      "214102,192.168.10.8,50305,23.194.108.67,80,6,5/7/2017 9:35,5559809,3,1,12,0.0,6,0,4.0,3.464101615,0,0,0.0,0.0,2.158347526,0.719449175,1853269.667,3189322.073,5535956.0,39.0,5559809.0,2779904.5,3897645.41,5535956.0,23853.0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,72,32,0.539586881,0.179862294,0,6,2.4,3.286335345,10.8,0,0,0,1,0,0,0,0,0,3.0,4.0,0.0,72,0,0,0,0,0,0,3,12,1,0,8192,29200,2,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "36502,172.217.11.3,80,192.168.10.8,49917,6,5/7/2017 9:31,18,1,1,6,6.0,6,6,6.0,0.0,6,6,6.0,0.0,666666.6667,111111.1111,18.0,0.0,18.0,18.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,20,20,55555.55556,55555.55556,6,6,6.0,0.0,0.0,0,0,0,0,1,1,0,0,1,9.0,6.0,6.0,20,0,0,0,0,0,0,1,6,1,6,343,16560,0,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "24975,192.168.10.50,80,172.16.0.1,51484,6,5/7/2017 11:00,12859,1,3,0,18.0,0,0,0.0,0.0,6,6,6.0,0.0,1399.797807,311.0661793,4286.333333,6734.794825,12049.0,1.0,0.0,0.0,0.0,0.0,0.0,12050.0,6025.0,8519.2225,12049.0,1.0,0,0,0,0,32,60,77.76654483,233.2996345,0,6,3.6,3.286335345,10.8,0,0,0,0,1,0,0,0,3,4.5,0.0,6.0,32,0,0,0,0,0,0,1,0,3,18,235,0,0,32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "151291,192.168.10.19,27610,192.168.10.3,53,17,5/7/2017 3:08,231,2,2,62,94.0,31,31,31.0,0.0,47,47,47.0,0.0,675324.6753,17316.01732,77.0,91.27978966,179.0,3.0,3.0,3.0,0.0,3.0,3.0,49.0,49.0,0.0,49.0,49.0,0,0,0,0,40,40,8658.008658,8658.008658,31,47,37.4,8.76356092,76.8,0,0,0,0,0,0,0,0,1,46.75,31.0,47.0,40,0,0,0,0,0,0,2,62,2,94,-1,-1,1,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n"
     ]
    }
   ],
   "source": [
    "# quick view of dataset\n",
    "!head -n5 {IDS_DATASET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv(IDS_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant features and label name\n",
    "df = df[RELEVANT_FEATURES + [LABEL_NAME]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract bengin and attack flows we want\n",
    "benign_df, attack_df = df[(df[LABEL_NAME] == BENIGN_LABEL)], df[(df[LABEL_NAME] == ATTACK_LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kc3172/py3tf/lib/python3.6/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# kc swap the label values \n",
    "\n",
    "# rewrite label values\n",
    "benign_df.loc[:, LABEL_NAME] = 1\n",
    "attack_df.loc[:, LABEL_NAME] = 0\n",
    "\n",
    "\n",
    "# benign_df.loc[:, LABEL_NAME] = 0\n",
    "# attack_df.loc[:, LABEL_NAME] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "benign, attack = benign_df.values, attack_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max normalization\n",
    "benign[:, :len(RELEVANT_FEATURES)] = max_norm(benign[:, :len(RELEVANT_FEATURES)])\n",
    "attack[:, :len(RELEVANT_FEATURES)] = max_norm(attack[:, :len(RELEVANT_FEATURES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.67757394e-01, 1.24760226e-03, 4.63374410e-02, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [1.22096395e-03, 7.78457028e-01, 1.58352782e-07, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [1.22096395e-03, 8.02894437e-01, 1.07179830e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [9.02124477e-01, 8.26536500e-04, 3.96340344e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [9.80205122e-01, 8.26536500e-04, 1.73354624e-06, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [7.76365190e-01, 8.26536500e-04, 5.32548739e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kc3172/py3tf/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# do train, test split separately on benign and attack\n",
    "benign_train, benign_test = train_test_split(benign, train_size=TRAIN_FRAC)\n",
    "attack_train, attack_test = train_test_split(attack, train_size=TRAIN_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat to get testing data\n",
    "test_np = np.concatenate([benign_test, attack_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to benign train dataset and attack train dataset\n",
    "benign_train_dataset, attack_train_dataset = tf.data.Dataset.from_tensor_slices(benign_train), tf.data.Dataset.from_tensor_slices(attack_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign example features: tf.Tensor(\n",
      "[7.90635206e-01 6.90859754e-03 2.31437591e-03 2.87368788e-02\n",
      " 1.20855615e-05 3.98972603e-02 3.44234079e-03 1.19703045e-02\n",
      " 2.70774177e-04 5.06063212e-04 7.37508468e-04 3.12012476e-08\n",
      " 1.56197500e-03 2.64741525e-04 6.46195823e-04 1.00000000e-06\n",
      " 1.58887500e-03 1.79533898e-04 6.37262987e-04 8.47457627e-09\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.25473073e-04\n",
      " 3.60113796e-05 2.12220541e-01 0.00000000e+00 1.42857143e-01\n",
      " 2.69327912e-01 4.25473073e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.21750322e-04 2.87368788e-02\n",
      " 4.45571899e-01 3.26228795e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00], shape=(41,), dtype=float64)\n",
      "benign example label: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# deal with benign train dataset\n",
    "benign_train_dataset = benign_train_dataset.map(parse_feature_label)\n",
    "benign_train_dataset = benign_train_dataset.shuffle(buffer_size=benign_train.shape[0] * 5)  # randomize\n",
    "benign_train_dataset = benign_train_dataset.batch(100) # make batch\n",
    "\n",
    "# View a single example entry from a batch\n",
    "benign_features, benign_label = iter(benign_train_dataset).next()\n",
    "print(\"benign example features:\", benign_features[0])\n",
    "print(\"benign example label:\", benign_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack example features: tf.Tensor(\n",
      "[9.46286848e-01 0.00000000e+00 2.52223495e-08 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.92857154e-08 0.00000000e+00 2.54237290e-08 2.75573192e-04\n",
      " 2.52100842e-08 8.90585253e-08 0.00000000e+00 1.53668844e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.76923077e-02\n",
      " 0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 5.76923077e-02 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.59589041e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00], shape=(41,), dtype=float64)\n",
      "attack example label: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# deal with attack train dataset\n",
    "attack_train_dataset = attack_train_dataset.map(parse_feature_label)\n",
    "attack_train_dataset = attack_train_dataset.shuffle(buffer_size=attack_train.shape[0] * 5)  # randomize\n",
    "attack_train_dataset = attack_train_dataset.batch(100) # make batch\n",
    "\n",
    "# View a single example entry from a batch\n",
    "attack_features, attack_label = iter(attack_train_dataset).next()\n",
    "print(\"attack example features:\", attack_features[0])\n",
    "print(\"attack example label:\", attack_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_shape=len(RELEVANT_FEATURES), output_shape=FEATURE_NUM_MODIFIED)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kc change lr rate\n",
    "\n",
    "# generator_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE,\n",
    "                                             beta1=0.9,beta2=0.999,epsilon=1e-08)\n",
    "# discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE,\n",
    "                                                 beta1=0.9,beta2=0.999,epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Tensorflow Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.eager.python.checkpointable_utils.InitializationOnlyStatus at 0x2aec19d51710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_counter = tf.train.get_or_create_global_step()\n",
    "summary_writer = tf.contrib.summary.create_file_writer(\n",
    "      OUTPUT_DIR, flush_millis=1000)\n",
    "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, 'ckpt')\n",
    "latest_cpkt = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
    "if latest_cpkt:\n",
    "    print('Using latest checkpoint at ' + latest_cpkt)\n",
    "model_objects = {\n",
    "    'generator': generator,\n",
    "    'discriminator': discriminator,\n",
    "    'generator_optimizer': generator_optimizer,\n",
    "    'discriminator_optimizer': discriminator_optimizer,\n",
    "    'step_counter': step_counter\n",
    "}\n",
    "checkpoint = tfe.Checkpoint(**model_objects)\n",
    "# Restore variables on creation if a checkpoint exists.\n",
    "checkpoint.restore(latest_cpkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #10\tAverage Generator Loss: 0.398015\tAverage Discriminator Loss: 1.513136\n",
      "\n",
      "Train time for epoch #1 (step 1): 1.886241\n",
      "Batch #10\tAverage Generator Loss: 0.404432\tAverage Discriminator Loss: 1.501496\n",
      "\n",
      "Train time for epoch #2 (step 2): 1.527615\n",
      "Batch #10\tAverage Generator Loss: 0.399929\tAverage Discriminator Loss: 1.511540\n",
      "\n",
      "Train time for epoch #3 (step 3): 1.473655\n",
      "Batch #10\tAverage Generator Loss: 0.407059\tAverage Discriminator Loss: 1.498991\n",
      "\n",
      "Train time for epoch #4 (step 4): 1.653267\n",
      "Batch #10\tAverage Generator Loss: 0.411132\tAverage Discriminator Loss: 1.492152\n",
      "\n",
      "Train time for epoch #5 (step 5): 1.619736\n",
      "Batch #10\tAverage Generator Loss: 0.412949\tAverage Discriminator Loss: 1.489809\n",
      "\n",
      "Train time for epoch #6 (step 6): 1.479452\n",
      "Batch #10\tAverage Generator Loss: 0.419343\tAverage Discriminator Loss: 1.478836\n",
      "\n",
      "Train time for epoch #7 (step 7): 1.634905\n",
      "Batch #10\tAverage Generator Loss: 0.418326\tAverage Discriminator Loss: 1.481796\n",
      "\n",
      "Train time for epoch #8 (step 8): 1.624617\n",
      "Batch #10\tAverage Generator Loss: 0.420657\tAverage Discriminator Loss: 1.478360\n",
      "\n",
      "Train time for epoch #9 (step 9): 1.660329\n",
      "Batch #10\tAverage Generator Loss: 0.420291\tAverage Discriminator Loss: 1.481259\n",
      "\n",
      "Train time for epoch #10 (step 10): 1.626786\n",
      "Batch #10\tAverage Generator Loss: 0.432822\tAverage Discriminator Loss: 1.458333\n",
      "\n",
      "Train time for epoch #11 (step 11): 1.521909\n",
      "Batch #10\tAverage Generator Loss: 0.431198\tAverage Discriminator Loss: 1.462698\n",
      "\n",
      "Train time for epoch #12 (step 12): 1.602983\n",
      "Batch #10\tAverage Generator Loss: 0.430253\tAverage Discriminator Loss: 1.466234\n",
      "\n",
      "Train time for epoch #13 (step 13): 1.579099\n",
      "Batch #10\tAverage Generator Loss: 0.433172\tAverage Discriminator Loss: 1.461926\n",
      "\n",
      "Train time for epoch #14 (step 14): 1.546475\n",
      "Batch #10\tAverage Generator Loss: 0.441428\tAverage Discriminator Loss: 1.448043\n",
      "\n",
      "Train time for epoch #15 (step 15): 1.616978\n",
      "Batch #10\tAverage Generator Loss: 0.446842\tAverage Discriminator Loss: 1.440250\n",
      "\n",
      "Train time for epoch #16 (step 16): 1.579244\n",
      "Batch #10\tAverage Generator Loss: 0.452610\tAverage Discriminator Loss: 1.430875\n",
      "\n",
      "Train time for epoch #17 (step 17): 1.612302\n",
      "Batch #10\tAverage Generator Loss: 0.456041\tAverage Discriminator Loss: 1.425849\n",
      "\n",
      "Train time for epoch #18 (step 18): 1.594891\n",
      "Batch #10\tAverage Generator Loss: 0.452003\tAverage Discriminator Loss: 1.434732\n",
      "\n",
      "Train time for epoch #19 (step 19): 1.557709\n",
      "Batch #10\tAverage Generator Loss: 0.465717\tAverage Discriminator Loss: 1.411883\n",
      "\n",
      "Train time for epoch #20 (step 20): 1.537429\n",
      "Batch #10\tAverage Generator Loss: 0.461508\tAverage Discriminator Loss: 1.420785\n",
      "\n",
      "Train time for epoch #21 (step 21): 1.579625\n",
      "Batch #10\tAverage Generator Loss: 0.471557\tAverage Discriminator Loss: 1.405369\n",
      "\n",
      "Train time for epoch #22 (step 22): 1.564655\n",
      "Batch #10\tAverage Generator Loss: 0.469684\tAverage Discriminator Loss: 1.408778\n",
      "\n",
      "Train time for epoch #23 (step 23): 1.554565\n",
      "Batch #10\tAverage Generator Loss: 0.477235\tAverage Discriminator Loss: 1.398241\n",
      "\n",
      "Train time for epoch #24 (step 24): 1.531065\n",
      "Batch #10\tAverage Generator Loss: 0.474497\tAverage Discriminator Loss: 1.403570\n",
      "\n",
      "Train time for epoch #25 (step 25): 1.609443\n",
      "Batch #10\tAverage Generator Loss: 0.485248\tAverage Discriminator Loss: 1.387571\n",
      "\n",
      "Train time for epoch #26 (step 26): 1.552374\n",
      "Batch #10\tAverage Generator Loss: 0.476379\tAverage Discriminator Loss: 1.403171\n",
      "\n",
      "Train time for epoch #27 (step 27): 1.560624\n",
      "Batch #10\tAverage Generator Loss: 0.483342\tAverage Discriminator Loss: 1.393422\n",
      "\n",
      "Train time for epoch #28 (step 28): 1.726444\n",
      "Batch #10\tAverage Generator Loss: 0.490934\tAverage Discriminator Loss: 1.380975\n",
      "\n",
      "Train time for epoch #29 (step 29): 1.557781\n",
      "Batch #10\tAverage Generator Loss: 0.491494\tAverage Discriminator Loss: 1.381237\n",
      "\n",
      "Train time for epoch #30 (step 30): 1.748208\n",
      "Batch #10\tAverage Generator Loss: 0.502443\tAverage Discriminator Loss: 1.366324\n",
      "\n",
      "Train time for epoch #31 (step 31): 1.677818\n",
      "Batch #10\tAverage Generator Loss: 0.499845\tAverage Discriminator Loss: 1.369757\n",
      "\n",
      "Train time for epoch #32 (step 32): 1.582554\n",
      "Batch #10\tAverage Generator Loss: 0.500872\tAverage Discriminator Loss: 1.368535\n",
      "\n",
      "Train time for epoch #33 (step 33): 1.741244\n",
      "Batch #10\tAverage Generator Loss: 0.513125\tAverage Discriminator Loss: 1.351096\n",
      "\n",
      "Train time for epoch #34 (step 34): 1.510637\n",
      "Batch #10\tAverage Generator Loss: 0.507573\tAverage Discriminator Loss: 1.360463\n",
      "\n",
      "Train time for epoch #35 (step 35): 1.606771\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    train_start = time.time()\n",
    "    for _ in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        with summary_writer.as_default():\n",
    "            train_one_epoch(benign_dataset=benign_train_dataset,\n",
    "                        attack_dataset=attack_train_dataset,\n",
    "                        log_interval=LOG_INTERVAL,\n",
    "                        modified_feature_num=FEATURE_NUM_MODIFIED,\n",
    "                        random_select_feature=RANDOM_SELECT_FEATURE,\n",
    "                        **model_objects)\n",
    "        end = time.time()\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "        print('\\nTrain time for epoch #%d (step %d): %f' %\n",
    "            (checkpoint.save_counter.numpy(),\n",
    "             checkpoint.step_counter.numpy(),\n",
    "             end - start))\n",
    "    print('\\nTotal training time for {epoch} epoch(s) is {second}'.format(\n",
    "        second=time.time() - train_start,\n",
    "        epoch=EPOCHS\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_DIR, 'benign_train.npy'), benign_train)\n",
    "np.save(os.path.join(DATA_DIR, 'benign_test.npy'), benign_test)\n",
    "np.save(os.path.join(DATA_DIR, 'attack_train.npy'), attack_train)\n",
    "np.save(os.path.join(DATA_DIR, 'attack_test.npy'), attack_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_benign_train = np.load(os.path.join(DATA_DIR, 'benign_train.npy'))\n",
    "loaded_benign_test = np.load(os.path.join(DATA_DIR, 'benign_test.npy'))\n",
    "loaded_attack_train = np.load(os.path.join(DATA_DIR, 'attack_train.npy'))\n",
    "loaded_attack_test = np.load(os.path.join(DATA_DIR, 'attack_test.npy'))\n",
    "assert (loaded_benign_train == benign_train).all()\n",
    "assert (loaded_benign_test == benign_test).all()\n",
    "assert (loaded_attack_train == attack_train).all()\n",
    "assert (loaded_attack_test == attack_test).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         